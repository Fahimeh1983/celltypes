{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing coupled word2vec idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this implementation which is specific for the directed graph, we take one node before and one node after a specific node. For example lets assume we have a walk from i to j to k. In our case, this means that i has emmited and j has received a signal. Then j has emitted and k has received a signal. We will try to reconstruct two embeddings, one for emitters and one for receivers and we call them E and R correspondingly. In this implementation, we will give j as input and will try to predict i on one arm of the machine and k on another arm. Then in the loss function we will try to minimize the distance between the emmitor representation of i and receiver representation of j as well as the distance between emmitor representation of j and receivor representation of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import torch.nn as nn\n",
    "from cell import graph_utils, utils\n",
    "from cell.Word2vec import prepare_vocab, dataloader, wv\n",
    "from stellargraph import StellarGraph\n",
    "from torch.nn import functional as F\n",
    "from stellargraph.data import BiasedRandomWalk\n",
    "import cell.BiasedDirectedWeightedWalk as BDWW\n",
    "from stellargraph import StellarDiGraph\n",
    "\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'cell.Word2vec.prepare_vocab' from '/Users/fahimehb/Documents/git-workspace/celltypes/cell/Word2vec/prepare_vocab.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imp\n",
    "from cell import  utils, analysis, plot_utils\n",
    "from cell.Word2vec import prepare_vocab, dataloader, wv\n",
    "\n",
    "\n",
    "imp.reload(utils)\n",
    "imp.reload(prepare_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, *datasets):\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return tuple(d[i] for d in self.datasets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(d) for d in self.datasets)\n",
    "    \n",
    "    \n",
    "def build_data_loader(datasets, batch_size, shuffle=True, drop_last=True, num_workers=1):\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        ConcatDataset(*[datasets[k][0] for k in datasets.keys()]),\n",
    "        batch_size=batch_size, \n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers)\n",
    "    return {k:i for i,k in enumerate(datasets.keys())}, data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmitterReceiver_Word2Vec(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size=[93], embedding_size=2, n_arm=1):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super(EmitterReceiver_Word2Vec, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.n_arm = n_arm\n",
    "        \n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(vocab_size[i],\n",
    "                                                      embedding_size) \n",
    "                                         for i in range(n_arm)])\n",
    "        \n",
    "        self.linear = nn.ModuleList([nn.Linear(embedding_size,\n",
    "                                               vocab_size[i]) \n",
    "                                     for i in range(n_arm)])\n",
    "        \n",
    "#         self.batch_norm = nn.ModuleList([nn.BatchNorm1d(num_features=embedding_size,\n",
    "#                                                         eps=1e-10, \n",
    "#                                                         momentum=0.1, \n",
    "#                                                         affine=False) \n",
    "#                                          for i in range(n_arm)])\n",
    "                        \n",
    "\n",
    "    def encoder(self, context_word, arm):\n",
    "        h1 = self.embeddings[arm](context_word)\n",
    "        node_embeddings = [self.embeddings[arm](torch.tensor(i)) for i \n",
    "                           in range(self.vocab_size[arm])]\n",
    "        return node_embeddings, h1\n",
    "\n",
    "    def decoder(self, context_word_embedding_of_the_other_arm, arm):\n",
    "        h2 = self.linear[arm](context_word_embedding_of_the_other_arm)\n",
    "        return h2\n",
    "\n",
    "    def forward(self, context_word):\n",
    "        emb = [None] * self.n_arm\n",
    "        predictions = [None] * self.n_arm\n",
    "        context_word_embedding = [None] * self.n_arm\n",
    "        \n",
    "        for arm in range(self.n_arm):\n",
    "            node_embeddings, word_embedding  = self.encoder(context_word[arm], arm)\n",
    "            emb[arm] = node_embeddings\n",
    "            context_word_embedding[arm] = word_embedding\n",
    "            \n",
    "        for arm in range(self.n_arm):\n",
    "            which_arm = -1 * arm + 1\n",
    "#             which_arm = arm\n",
    "            predictions[arm] = self.decoder(context_word_embedding[which_arm], arm)\n",
    "            \n",
    "        return emb, predictions\n",
    "\n",
    "    \n",
    "def loss_emitter_receiver(prediction, target, n_arm, vocab_size, batch_size):\n",
    " \n",
    "    loss_indep = [None] * n_arm\n",
    "    \n",
    "    for arm, (k, v) in enumerate(arm_keys.items()):\n",
    "        predict[arm] = torch.reshape(prediction[arm], (batch_size, vocab_size))\n",
    "        loss_indep[arm] = F.cross_entropy(prediction[arm], target[arm])\n",
    "                \n",
    "    loss = sum(loss_indep)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load one walk on the test_all_combined communication network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 10000\n",
    "p = 1\n",
    "q = 1\n",
    "N = 1\n",
    "batch_size = 2000\n",
    "walk_filename = \"walk_node21_32_removed.csv\"\n",
    "roi = \"VISp\"\n",
    "project_name = \"NPP_GNN_project\"\n",
    "layer_class = \"single_layer\"\n",
    "layer = \"base_unnormalized_allcombined\"\n",
    "walk_type= \"Directed_Weighted_node2vec\"\n",
    "\n",
    "walk_dir = utils.get_walk_dir(roi,\n",
    "                              project_name, \n",
    "                              N, \n",
    "                              length, \n",
    "                              p, \n",
    "                              q, \n",
    "                              layer_class, \n",
    "                              layer, \n",
    "                              walk_type) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenght of vocabulary: 91\n"
     ]
    }
   ],
   "source": [
    "corpus = utils.read_list_of_lists_from_csv(os.path.join(walk_dir, walk_filename))\n",
    "vocabulary = prepare_vocab.get_vocabulary(corpus)\n",
    "\n",
    "print(f'lenght of vocabulary: {len(vocabulary)}')\n",
    "\n",
    "word_2_index = prepare_vocab.get_word2idx(vocabulary, padding=False)\n",
    "index_2_word = prepare_vocab.get_idx2word(vocabulary, padding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '71', '34', '10', '5', '35', '17', '59', '27', '14']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the context-target tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "receiver_tuples, emitter_tuples = prepare_vocab.emitter_receiver_tuples(corpus, window=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('34', '5'), ('34', '10'), ('10', '35'), ('10', '5'), ('5', '17'), ('5', '35'), ('35', '59'), ('35', '17'), ('17', '27'), ('17', '59')]\n",
      "[('34', '0'), ('34', '71'), ('10', '71'), ('10', '34'), ('5', '34'), ('5', '10'), ('35', '10'), ('35', '5'), ('17', '5'), ('17', '35')]\n"
     ]
    }
   ],
   "source": [
    "print(emitter_tuples[0:10])\n",
    "print(receiver_tuples[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "\n",
    "datasets['E'] = []\n",
    "emitter_dataset = dataloader.EmitterReceiverDataset(emitter_tuples, word_2_index)\n",
    "emitter_v_size = prepare_vocab.get_vocab_size_from_list_of_tuples(emitter_tuples)\n",
    "datasets['E'].append(emitter_dataset)\n",
    "datasets['E'].append(emitter_v_size)\n",
    "\n",
    "datasets['R'] = []\n",
    "receiver_dataset = dataloader.EmitterReceiverDataset(receiver_tuples, word_2_index)\n",
    "receiver_v_size = prepare_vocab.get_vocab_size_from_list_of_tuples(receiver_tuples)\n",
    "datasets['R'].append(receiver_dataset)\n",
    "datasets['R'].append(receiver_v_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'E': [<cell.Word2vec.dataloader.EmitterReceiverDataset at 0x7fc669709d10>,\n",
       "  91],\n",
       " 'R': [<cell.Word2vec.dataloader.EmitterReceiverDataset at 0x7fc669709b10>,\n",
       "  91]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4,  4, 33, 33, 37, 37, 65, 65, 52, 52]) tensor([12,  9,  9,  4,  4, 33, 33, 37, 37, 65])\n"
     ]
    }
   ],
   "source": [
    "k = 10000\n",
    "kk = 10010\n",
    "print(datasets['R'][0].context[k:kk], datasets['R'][0].target[k:kk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4,  4, 33, 33, 37, 37, 65, 65, 52, 52]) tensor([37, 33, 65, 37, 52, 65, 66, 52, 44, 66])\n"
     ]
    }
   ],
   "source": [
    "print(datasets['E'][0].context[k:kk], datasets['E'][0].target[k:kk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 4, 33, 33, 37, 37, 65, 65, 52, 52]\n",
      "[37, 33, 65, 37, 52, 65, 66, 52, 44, 66]\n"
     ]
    }
   ],
   "source": [
    "print([word_2_index[i] for (i, j) in emitter_tuples[k:kk]])\n",
    "print([word_2_index[j] for (i, j) in emitter_tuples[k:kk]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 4, 33, 33, 37, 37, 65, 65, 52, 52]\n",
      "[12, 9, 9, 4, 4, 33, 33, 37, 37, 65]\n"
     ]
    }
   ],
   "source": [
    "print([word_2_index[i] for (i, j) in receiver_tuples[k:kk]])\n",
    "print([word_2_index[j] for (i, j) in receiver_tuples[k:kk]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('25', '83'),\n",
       " ('25', '27'),\n",
       " ('27', '50'),\n",
       " ('27', '83'),\n",
       " ('83', '26'),\n",
       " ('83', '50'),\n",
       " ('50', '45'),\n",
       " ('50', '26'),\n",
       " ('26', '41'),\n",
       " ('26', '45')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emitter_tuples[k:kk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "arm_keys, data_loader = build_data_loader(datasets, batch_size=2000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([56, 56,  8,  ..., 31, 86, 86]), tensor([53,  8, 90,  ..., 86, 17, 11])] [tensor([56, 56,  8,  ..., 31, 86, 86]), tensor([49, 32, 32,  ..., 81, 81, 31])]\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (data1, data2) in enumerate(data_loader):\n",
    "    print(data1, data2)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/50, loss:9.0636\n",
      "epoch: 2/50, loss:8.8933\n",
      "epoch: 3/50, loss:8.8767\n",
      "epoch: 4/50, loss:8.8614\n",
      "epoch: 5/50, loss:8.8536\n",
      "epoch: 6/50, loss:8.8503\n",
      "epoch: 7/50, loss:8.8484\n",
      "epoch: 8/50, loss:8.8472\n",
      "epoch: 9/50, loss:8.8464\n",
      "epoch: 10/50, loss:8.8461\n",
      "epoch: 11/50, loss:8.8459\n",
      "epoch: 12/50, loss:8.8457\n",
      "epoch: 13/50, loss:8.8457\n",
      "epoch: 14/50, loss:8.8457\n",
      "epoch: 15/50, loss:8.8456\n",
      "epoch: 16/50, loss:8.8456\n",
      "epoch: 17/50, loss:8.8456\n",
      "epoch: 18/50, loss:8.8456\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 91\n",
    "embedding_size = 2\n",
    "learning_rate = 0.001\n",
    "n_epochs = 50\n",
    "n_arm=2\n",
    "\n",
    "model = EmitterReceiver_Word2Vec(embedding_size=embedding_size, \n",
    "                        vocab_size=[v[1] for (k, v) in datasets.items()],\n",
    "                        n_arm=n_arm).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "training_loss = []\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    losses = []\n",
    "    t0 = time.time()\n",
    "    for batch_idx, all_data in enumerate(data_loader):\n",
    "        target_data = [data[1].to(device) for data in all_data]\n",
    "        context_data = [data[0].to(device) for data in all_data]\n",
    "        context_data = [torch.reshape(context_data[i], (batch_size, 1)) for i in range(len(context_data))]\n",
    "        optimizer.zero_grad()\n",
    "        emb, predict = model(context_data)\n",
    "        loss = loss_emitter_receiver(predict, target_data, n_arm, vocab_size, batch_size)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "#     t1 = time.time()\n",
    "#     print('time is %.2f' % (t1 - t0))\n",
    "        \n",
    "    training_loss.append(np.mean(losses)) \n",
    "    print(f'epoch: {epoch+1}/{n_epochs}, loss:{np.mean(losses):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cldf = utils.read_visp_npp_cldf()\n",
    "vectors = pd.DataFrame(model.embeddings[1].weight.detach().numpy(), index=index_2_word.values())\n",
    "vectors.index.name = \"cluster_id\"\n",
    "vectors.columns = [\"Z0\", \"Z1\"]\n",
    "cldf.index = cldf.index.astype(str)\n",
    "emb = vectors.merge(cldf, on=\"cluster_id\")\n",
    "fig = plot_utils.plot_embedding(data=emb, plot_dim=2, plot_size=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cldf = utils.read_visp_npp_cldf()\n",
    "vectors = pd.DataFrame(model.embeddings[0].weight.detach().numpy(), index=index_2_word.values())\n",
    "vectors.index.name = \"cluster_id\"\n",
    "vectors.columns = [\"Z0\", \"Z1\"]\n",
    "cldf.index = cldf.index.astype(str)\n",
    "emb = vectors.merge(cldf, on=\"cluster_id\")\n",
    "fig = plot_utils.plot_embedding(data=emb, plot_dim=2, plot_size=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py374",
   "language": "python",
   "name": "py374"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
