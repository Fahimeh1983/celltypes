{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "from cell import utils, analysis, plot_utils\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cell.Word2vec import prepare_vocab, dataloader, wv\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 10000\n",
    "p = 1\n",
    "q = 1\n",
    "N = 1\n",
    "batch_size = 2000\n",
    "walk_filename = \"walk_node21_32_removed.csv\"\n",
    "roi = \"VISp\"\n",
    "project_name = \"NPP_GNN_project\"\n",
    "layer_class = \"single_layer\"\n",
    "layer = \"base_unnormalized_allcombined\"\n",
    "walk_type= \"Directed_Weighted_node2vec\"\n",
    "window = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenght of vocabulary: 91\n",
      "a node called pad is added for padding and its index is zero\n",
      "a node called pad is added for padding and its index is zero\n",
      "MCBOW by default adds a padding node called pad with index zero\n",
      "There are 910000 pairs of target and context words\n",
      "lenght of vocabulary: 89\n",
      "a node called pad is added for padding and its index is zero\n",
      "a node called pad is added for padding and its index is zero\n",
      "MCBOW by default adds a padding node called pad with index zero\n",
      "There are 890000 pairs of target and context words\n",
      "lenght of vocabulary: 91\n",
      "a node called pad is added for padding and its index is zero\n",
      "a node called pad is added for padding and its index is zero\n",
      "MCBOW by default adds a padding node called pad with index zero\n",
      "There are 910000 pairs of target and context words\n",
      "lenght of vocabulary: 91\n",
      "a node called pad is added for padding and its index is zero\n",
      "a node called pad is added for padding and its index is zero\n",
      "MCBOW by default adds a padding node called pad with index zero\n",
      "There are 910000 pairs of target and context words\n",
      "lenght of vocabulary: 89\n",
      "a node called pad is added for padding and its index is zero\n",
      "a node called pad is added for padding and its index is zero\n",
      "MCBOW by default adds a padding node called pad with index zero\n",
      "There are 890000 pairs of target and context words\n"
     ]
    }
   ],
   "source": [
    "datasets = {}\n",
    "\n",
    "for (layer, walk_filename) in [(\"base_unnormalized_allcombined\", \"walk_node21_32_removed.csv\"),\n",
    "                               (\"Sst-Sstr1\", \"walk_0.csv\"),\n",
    "                               (\"Sst-Sstr2\", \"walk_0.csv\"),\n",
    "                               (\"Vip-Vipr1\", \"walk_0.csv\"),\n",
    "                               (\"Vip-Vipr2\", \"walk_0.csv\")]:\n",
    "    \n",
    "    walk_dir = utils.get_walk_dir(roi,\n",
    "                                  project_name, \n",
    "                                  N, \n",
    "                                  length, \n",
    "                                  p, \n",
    "                                  q, \n",
    "                                  layer_class, \n",
    "                                  layer, \n",
    "                                  walk_type) \n",
    "    path = os.path.join(walk_dir, walk_filename)\n",
    "    corpus = utils.read_list_of_lists_from_csv(path)\n",
    "    vocabulary = prepare_vocab.get_vocabulary(corpus)\n",
    "    \n",
    "    print(f'lenght of vocabulary: {len(vocabulary)}')\n",
    "    \n",
    "    word_2_index = prepare_vocab.get_word2idx(vocabulary, padding=True)\n",
    "    index_2_word = prepare_vocab.get_idx2word(vocabulary, padding=True)\n",
    "    datasets[layer] = [word_2_index]\n",
    "    datasets[layer].append(index_2_word)\n",
    "    \n",
    "    tuples = prepare_vocab.MCBOW_get_word_context_tuples(corpus, window=window)\n",
    "    dataset = dataloader.MCBOW_WalkDataset(tuples, word_2_index)\n",
    "    datasets[layer].append(dataset)\n",
    "    \n",
    "    datasets[layer].append(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'pad': 0,\n",
       "  '69': 1,\n",
       "  '6': 2,\n",
       "  '10': 3,\n",
       "  '84': 4,\n",
       "  '87': 5,\n",
       "  '31': 6,\n",
       "  '42': 7,\n",
       "  '90': 8,\n",
       "  '4': 9,\n",
       "  '37': 10,\n",
       "  '27': 11,\n",
       "  '38': 12,\n",
       "  '36': 13,\n",
       "  '46': 14,\n",
       "  '68': 15,\n",
       "  '89': 16,\n",
       "  '50': 17,\n",
       "  '58': 18,\n",
       "  '1': 19,\n",
       "  '12': 20,\n",
       "  '2': 21,\n",
       "  '49': 22,\n",
       "  '82': 23,\n",
       "  '74': 24,\n",
       "  '86': 25,\n",
       "  '81': 26,\n",
       "  '53': 27,\n",
       "  '3': 28,\n",
       "  '14': 29,\n",
       "  '17': 30,\n",
       "  '75': 31,\n",
       "  '40': 32,\n",
       "  '80': 33,\n",
       "  '43': 34,\n",
       "  '54': 35,\n",
       "  '52': 36,\n",
       "  '51': 37,\n",
       "  '91': 38,\n",
       "  '28': 39,\n",
       "  '59': 40,\n",
       "  '33': 41,\n",
       "  '57': 42,\n",
       "  '34': 43,\n",
       "  '7': 44,\n",
       "  '24': 45,\n",
       "  '67': 46,\n",
       "  '66': 47,\n",
       "  '29': 48,\n",
       "  '77': 49,\n",
       "  '55': 50,\n",
       "  '9': 51,\n",
       "  '8': 52,\n",
       "  '73': 53,\n",
       "  '23': 54,\n",
       "  '60': 55,\n",
       "  '47': 56,\n",
       "  '25': 57,\n",
       "  '76': 58,\n",
       "  '62': 59,\n",
       "  '72': 60,\n",
       "  '44': 61,\n",
       "  '13': 62,\n",
       "  '78': 63,\n",
       "  '0': 64,\n",
       "  '65': 65,\n",
       "  '79': 66,\n",
       "  '48': 67,\n",
       "  '70': 68,\n",
       "  '56': 69,\n",
       "  '30': 70,\n",
       "  '85': 71,\n",
       "  '39': 72,\n",
       "  '71': 73,\n",
       "  '5': 74,\n",
       "  '35': 75,\n",
       "  '20': 76,\n",
       "  '92': 77,\n",
       "  '64': 78,\n",
       "  '63': 79,\n",
       "  '83': 80,\n",
       "  '19': 81,\n",
       "  '41': 82,\n",
       "  '22': 83,\n",
       "  '61': 84,\n",
       "  '11': 85,\n",
       "  '45': 86,\n",
       "  '26': 87,\n",
       "  '16': 88,\n",
       "  '88': 89,\n",
       "  '18': 90,\n",
       "  '15': 91},\n",
       " {0: 'pad',\n",
       "  1: '69',\n",
       "  2: '6',\n",
       "  3: '10',\n",
       "  4: '84',\n",
       "  5: '87',\n",
       "  6: '31',\n",
       "  7: '42',\n",
       "  8: '90',\n",
       "  9: '4',\n",
       "  10: '37',\n",
       "  11: '27',\n",
       "  12: '38',\n",
       "  13: '36',\n",
       "  14: '46',\n",
       "  15: '68',\n",
       "  16: '89',\n",
       "  17: '50',\n",
       "  18: '58',\n",
       "  19: '1',\n",
       "  20: '12',\n",
       "  21: '2',\n",
       "  22: '49',\n",
       "  23: '82',\n",
       "  24: '74',\n",
       "  25: '86',\n",
       "  26: '81',\n",
       "  27: '53',\n",
       "  28: '3',\n",
       "  29: '14',\n",
       "  30: '17',\n",
       "  31: '75',\n",
       "  32: '40',\n",
       "  33: '80',\n",
       "  34: '43',\n",
       "  35: '54',\n",
       "  36: '52',\n",
       "  37: '51',\n",
       "  38: '91',\n",
       "  39: '28',\n",
       "  40: '59',\n",
       "  41: '33',\n",
       "  42: '57',\n",
       "  43: '34',\n",
       "  44: '7',\n",
       "  45: '24',\n",
       "  46: '67',\n",
       "  47: '66',\n",
       "  48: '29',\n",
       "  49: '77',\n",
       "  50: '55',\n",
       "  51: '9',\n",
       "  52: '8',\n",
       "  53: '73',\n",
       "  54: '23',\n",
       "  55: '60',\n",
       "  56: '47',\n",
       "  57: '25',\n",
       "  58: '76',\n",
       "  59: '62',\n",
       "  60: '72',\n",
       "  61: '44',\n",
       "  62: '13',\n",
       "  63: '78',\n",
       "  64: '0',\n",
       "  65: '65',\n",
       "  66: '79',\n",
       "  67: '48',\n",
       "  68: '70',\n",
       "  69: '56',\n",
       "  70: '30',\n",
       "  71: '85',\n",
       "  72: '39',\n",
       "  73: '71',\n",
       "  74: '5',\n",
       "  75: '35',\n",
       "  76: '20',\n",
       "  77: '92',\n",
       "  78: '64',\n",
       "  79: '63',\n",
       "  80: '83',\n",
       "  81: '19',\n",
       "  82: '41',\n",
       "  83: '22',\n",
       "  84: '61',\n",
       "  85: '11',\n",
       "  86: '45',\n",
       "  87: '26',\n",
       "  88: '16',\n",
       "  89: '88',\n",
       "  90: '18',\n",
       "  91: '15'},\n",
       " <cell.Word2vec.dataloader.MCBOW_WalkDataset at 0x7fdd70943e50>,\n",
       " 91]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['base_unnormalized_allcombined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_intersections(datasets, base_layer_name):\n",
    "    node_intersections = {}\n",
    "    for k, v in datasets.items():\n",
    "        l1 = set(datasets[k][0])\n",
    "        l2 = set(datasets[base_layer_name][0])\n",
    "        node_intersections[k] = set(l1).intersection(l2)\n",
    "    \n",
    "    return node_intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['base_unnormalized_allcombined', 'Sst-Sstr1', 'Sst-Sstr2', 'Vip-Vipr1', 'Vip-Vipr2'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_layer_name = \"base_unnormalized_allcombined\"\n",
    "layers = [\"Sst-Sstr1\", \"Sst-Sstr2\", \"Vip-Vipr1\", \"Vip-Vipr2\"]\n",
    "\n",
    "node_intersections = get_node_intersections(datasets, base_layer_name)\n",
    "node_intersections.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, *datasets):\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return tuple(d[i] for d in self.datasets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(d) for d in self.datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['base_unnormalized_allcombined', 'Sst-Sstr1', 'Sst-Sstr2', 'Vip-Vipr1', 'Vip-Vipr2'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cell.Word2vec.dataloader.MCBOW_WalkDataset at 0x7fdd70943e50>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['base_unnormalized_allcombined'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_loader(datasets, batch_size, shuffle=True, drop_last=True, num_workers=1):\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        ConcatDataset(*[datasets[k][2] for k in datasets.keys()]),\n",
    "        batch_size=batch_size, \n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers)\n",
    "    return {k:i for i,k in enumerate(datasets.keys())}, data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "arm_keys, data_loader = build_data_loader(datasets, batch_size=2000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([64, 14, 37,  ..., 76, 17, 76]), tensor([[ 0,  0, 14, 37],\n",
      "        [ 0, 64, 37, 35],\n",
      "        [64, 14, 35, 76],\n",
      "        ...,\n",
      "        [76, 76, 17, 76],\n",
      "        [76, 76, 76, 17],\n",
      "        [76, 17, 17, 76]])]\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (data1, data2, data3, data4, data4) in enumerate(data_loader):\n",
    "    print(data4)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take care of index in different arms and different number of nodes in different arms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_unnormalized_allcombined': 0,\n",
       " 'Sst-Sstr1': 1,\n",
       " 'Sst-Sstr2': 2,\n",
       " 'Vip-Vipr1': 3,\n",
       " 'Vip-Vipr2': 4}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arm_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v_0 = pd.DataFrame(torch.stack(emb[0]).detach().numpy(), \n",
    "#                    index=datasets['base_unnormalized_allcombined'][1].values())\n",
    "\n",
    "# v_1 = pd.DataFrame(torch.stack(emb[1]).detach().numpy(), \n",
    "#                    index=datasets['Sst-Sstr1'][1].values())\n",
    "\n",
    "# v_0.index.name = \"cluster_id\"\n",
    "# v_1.index.name = \"cluster_id\"\n",
    "\n",
    "# merged = v_1.merge(v_0, on='cluster_id')\n",
    "# v_0 = merged[['0_x', '1_x']]\n",
    "# v_1 = merged[['0_y', '1_y']]\n",
    "\n",
    "# v_0 = torch.tensor(np.array(v_0))\n",
    "# v_1 = torch.tensor(np.array(v_1))\n",
    "# F.mse_loss(v_0, v_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_joint = 0 \n",
    "\n",
    "# base_arm = arm_keys[base_layer_name]\n",
    "# for arm, (k, v) in enumerate(arm_keys.items()):\n",
    "#     print(arm, k, v)\n",
    "#     idx0 = [datasets[base_layer_name][0][i] for i in node_intersections[k]]\n",
    "#     idx1 = [datasets[k][0][i] for i in node_intersections[k]]\n",
    "#     loss_joint += F.mse_loss(torch.index_select(input=torch.stack(emb[v]), \n",
    "#                                                 dim=0, \n",
    "#                                                 index=torch.tensor(idx1), \n",
    "#                                                 out=None),\n",
    "#                              torch.index_select(input=torch.stack(emb[base_arm]), \n",
    "#                                                 dim=0, \n",
    "#                                                 index=torch.tensor(idx0), \n",
    "#                                                 out=None))\n",
    "# print(loss_joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_CMCBOW(prediction, target, emb, arm_keys, base_layer_name, node_intersections, n_arm=2):\n",
    "    \n",
    "    base_arm = arm_keys[base_layer_name]\n",
    "    loss_indep = [None] * n_arm\n",
    "    loss_joint = [None] * n_arm\n",
    "    \n",
    "    for arm, (k, v) in enumerate(arm_keys.items()):\n",
    "        \n",
    "        loss_indep[arm] = F.cross_entropy(prediction[arm], target[arm])\n",
    "        \n",
    "        idx0 = [datasets[base_layer_name][0][i] for i in node_intersections[k]]\n",
    "        idx1 = [datasets[k][0][i] for i in node_intersections[k]]\n",
    "        loss_joint[arm] = F.mse_loss(torch.index_select(input=torch.stack(emb[v]), \n",
    "                                                    dim=0, \n",
    "                                                    index=torch.tensor(idx1),\n",
    "                                                    out=None),\n",
    "                                 torch.index_select(input=torch.stack(emb[base_arm]), \n",
    "                                                    dim=0, \n",
    "                                                    index=torch.tensor(idx0), \n",
    "                                                    out=None))\n",
    "    loss = sum(loss_indep) + sum(loss_joint)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn import functional as F\n",
    "\n",
    "# i = 10\n",
    "# arm = 2\n",
    "# print(F.cross_entropy(predict[arm][[i]], target_data[arm][[i]]))\n",
    "\n",
    "# sf = F.softmax(predict[arm][i], dim=0)\n",
    "# loss = -1 * torch.log(sf)\n",
    "# print(loss[target_data[arm][i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coupled MCBOW_Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CMCBOW_Word2Vec(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size=[93], embedding_size=2, n_arm=1, padding_idx=0):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super(CMCBOW_Word2Vec, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.n_arm = n_arm\n",
    "        \n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(vocab_size[i],\n",
    "                                                      embedding_size, \n",
    "                                                      padding_idx=padding_idx) \n",
    "                                         for i in range(n_arm)])\n",
    "        \n",
    "        self.linear = nn.ModuleList([nn.Linear(embedding_size,\n",
    "                                               vocab_size[i]) \n",
    "                                     for i in range(n_arm)])\n",
    "        \n",
    "        self.batch_norm = nn.ModuleList([nn.BatchNorm1d(num_features=embedding_size,\n",
    "                                                        eps=1e-10, \n",
    "                                                        momentum=0.1, \n",
    "                                                        affine=False) \n",
    "                                         for i in range(n_arm)])\n",
    "                        \n",
    "\n",
    "    def encoder(self, context_words, arm):\n",
    "        h1 = torch.mean(self.embeddings[arm](context_words), dim=1)\n",
    "        node_embeddings = [self.embeddings[arm](torch.tensor(i)) for i \n",
    "                           in range(self.vocab_size[arm])]\n",
    "        \n",
    "        return node_embeddings ,h1\n",
    "\n",
    "    def decoder(self, mean_context, arm):\n",
    "        h2 = self.linear[arm](self.batch_norm[arm](mean_context))\n",
    "        return h2\n",
    "\n",
    "    def forward(self, context_words):\n",
    "        emb = [None] * self.n_arm\n",
    "        predictions = [None] * self.n_arm\n",
    "\n",
    "        for arm in range(self.n_arm):\n",
    "            node_embeddings , mean_context = self.encoder(context_words[arm], arm)\n",
    "            emb[arm] = node_embeddings\n",
    "            predictions[arm] = self.decoder(mean_context, arm)\n",
    "            \n",
    "        return emb, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 2\n",
    "learning_rate = 0.001\n",
    "n_epochs = 1\n",
    "n_arm=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time is 91.76\n",
      "epoch: 1/1, loss:26.5933\n"
     ]
    }
   ],
   "source": [
    "model = CMCBOW_Word2Vec(embedding_size=embedding_size, \n",
    "                        vocab_size=[v[3] + 1 for (k, v) in datasets.items()],\n",
    "                        n_arm=n_arm, \n",
    "                        padding_idx=0).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "training_loss = []\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    losses = []\n",
    "    t0 = time.time()\n",
    "    for batch_idx, all_data in enumerate(data_loader):\n",
    "        target_data = [data[0].to(device) for data in all_data]\n",
    "        context_data = [data[1].to(device) for data in all_data]\n",
    "        optimizer.zero_grad()\n",
    "        emb, predict = model(context_data)\n",
    "        loss = loss_CMCBOW(prediction=predict, \n",
    "                           target=target_data, \n",
    "                           arm_keys=arm_keys, \n",
    "                           emb=emb, \n",
    "                           n_arm=n_arm, \n",
    "                           base_layer_name=base_layer_name, \n",
    "                           node_intersections=node_intersections) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    t1 = time.time()\n",
    "    print('time is %.2f' % (t1 - t0))\n",
    "        \n",
    "    training_loss.append(np.mean(losses)) \n",
    "    print(f'epoch: {epoch+1}/{n_epochs}, loss:{np.mean(losses):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([21,  9,  4,  ..., 39, 58, 67]),\n",
       " tensor([56, 56, 56,  ..., 56, 56, 56]),\n",
       " tensor([61, 61, 61,  ..., 61, 61, 61]),\n",
       " tensor([91, 91, 91,  ..., 91, 91, 91]),\n",
       " tensor([76, 37, 67,  ..., 28, 37, 76])]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(22.7799, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 92])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading cldf from: //Users/fahimehb/Documents/NPP_GNN_project/dat/cl_df_VISp_annotation.csv\n"
     ]
    }
   ],
   "source": [
    "cldf = utils.read_visp_npp_cldf()\n",
    "vectors = model.embeddings[0].weight.detach().numpy()\n",
    "\n",
    "data = analysis.summarize_walk_embedding_results(gensim_dict={\"model\": vectors},\n",
    "                                                 index=index_2_word.values(),\n",
    "                                                 ndim=2, \n",
    "                                                 cl_df=cldf, \n",
    "                                                 padding_label=\"pad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = utils.get_model_dir(project_name, \n",
    "                                roi, \n",
    "                                N, \n",
    "                                length, \n",
    "                                p, \n",
    "                                q, \n",
    "                                layer_class, \n",
    "                                layer, \n",
    "                                walk_type)\n",
    "\n",
    "model_name = utils.get_model_name(size=embedding_size, \n",
    "                                  iter=n_epochs, \n",
    "                                  window=2, \n",
    "                                  lr=learning_rate, \n",
    "                                  batch_size=batch_size,\n",
    "                                  opt_add=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '//Users/fahimehb/Documents/NPP_GNN_project/models/VISp/single_layer/Directed_Weighted_node2vec/N_1_l_10000_p_1_q_1/Vip-Vipr1/model_size_2_iter_10_window_2_lr_0.001_bs_2000_test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-77ce4d9d94e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py374/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   3226\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3227\u001b[0m         )\n\u001b[0;32m-> 3228\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py374/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             )\n\u001b[1;32m    185\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py374/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0;31m# No explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '//Users/fahimehb/Documents/NPP_GNN_project/models/VISp/single_layer/Directed_Weighted_node2vec/N_1_l_10000_p_1_q_1/Vip-Vipr1/model_size_2_iter_10_window_2_lr_0.001_bs_2000_test.csv'"
     ]
    }
   ],
   "source": [
    "data.to_csv(os.path.join(model_dir, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_size_2_iter_10_window_2_lr_0.001_bs_2000_test.csv'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py374",
   "language": "python",
   "name": "py374"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
