{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "from cell import utils, analysis, plot_utils\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cell.Word2vec import prepare_vocab, dataloader, wv\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 10000\n",
    "p = 1\n",
    "q = 1\n",
    "N = 1\n",
    "batch_size = 2000\n",
    "walk_filename = \"walk_node21_32_removed.csv\"\n",
    "roi = \"VISp\"\n",
    "project_name = \"NPP_GNN_project\"\n",
    "layer_class = \"single_layer\"\n",
    "layer = \"base_unnormalized_allcombined\"\n",
    "walk_type= \"Directed_Weighted_node2vec\"\n",
    "window = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenght of vocabulary: 91\n",
      "a node called pad is added for padding and its index is zero\n",
      "a node called pad is added for padding and its index is zero\n",
      "MCBOW by default adds a padding node called pad with index zero\n",
      "There are 910000 pairs of target and context words\n",
      "lenght of vocabulary: 89\n",
      "a node called pad is added for padding and its index is zero\n",
      "a node called pad is added for padding and its index is zero\n",
      "MCBOW by default adds a padding node called pad with index zero\n",
      "There are 890000 pairs of target and context words\n",
      "lenght of vocabulary: 91\n",
      "a node called pad is added for padding and its index is zero\n",
      "a node called pad is added for padding and its index is zero\n",
      "MCBOW by default adds a padding node called pad with index zero\n",
      "There are 910000 pairs of target and context words\n",
      "lenght of vocabulary: 91\n",
      "a node called pad is added for padding and its index is zero\n",
      "a node called pad is added for padding and its index is zero\n",
      "MCBOW by default adds a padding node called pad with index zero\n",
      "There are 910000 pairs of target and context words\n",
      "lenght of vocabulary: 89\n",
      "a node called pad is added for padding and its index is zero\n",
      "a node called pad is added for padding and its index is zero\n",
      "MCBOW by default adds a padding node called pad with index zero\n",
      "There are 890000 pairs of target and context words\n"
     ]
    }
   ],
   "source": [
    "datasets = {}\n",
    "\n",
    "for (layer, walk_filename) in [(\"base_unnormalized_allcombined\", \"walk_node21_32_removed.csv\"),\n",
    "                               (\"Sst-Sstr1\", \"walk_0.csv\"),\n",
    "                               (\"Sst-Sstr2\", \"walk_0.csv\"),\n",
    "                               (\"Vip-Vipr1\", \"walk_0.csv\"),\n",
    "                               (\"Vip-Vipr2\", \"walk_0.csv\")]:\n",
    "    \n",
    "    walk_dir = utils.get_walk_dir(roi,\n",
    "                                  project_name, \n",
    "                                  N, \n",
    "                                  length, \n",
    "                                  p, \n",
    "                                  q, \n",
    "                                  layer_class, \n",
    "                                  layer, \n",
    "                                  walk_type) \n",
    "    path = os.path.join(walk_dir, walk_filename)\n",
    "    corpus = utils.read_list_of_lists_from_csv(path)\n",
    "    vocabulary = prepare_vocab.get_vocabulary(corpus)\n",
    "    \n",
    "    print(f'lenght of vocabulary: {len(vocabulary)}')\n",
    "    \n",
    "    word_2_index = prepare_vocab.get_word2idx(vocabulary, padding=True)\n",
    "    index_2_word = prepare_vocab.get_idx2word(vocabulary, padding=True)\n",
    "    datasets[layer] = [word_2_index]\n",
    "    datasets[layer].append(index_2_word)\n",
    "    \n",
    "    tuples = prepare_vocab.MCBOW_get_word_context_tuples(corpus, window=window)\n",
    "    dataset = dataloader.MCBOW_WalkDataset(tuples, word_2_index)\n",
    "    datasets[layer].append(dataset)\n",
    "    \n",
    "    datasets[layer].append(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_intersections(datasets, base_layer_name):\n",
    "    node_intersections = {}\n",
    "    for k, v in datasets.items():\n",
    "        l1 = set(datasets[k][0])\n",
    "        l2 = set(datasets[base_layer_name][0])\n",
    "        node_intersections[k] = set(l1).intersection(l2)\n",
    "    \n",
    "    return node_intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['base_unnormalized_allcombined', 'Sst-Sstr1', 'Sst-Sstr2', 'Vip-Vipr1', 'Vip-Vipr2'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_layer_name = \"base_unnormalized_allcombined\"\n",
    "layers = [\"Sst-Sstr1\", \"Sst-Sstr2\", \"Vip-Vipr1\", \"Vip-Vipr2\"]\n",
    "\n",
    "node_intersections = get_node_intersections(datasets, base_layer_name)\n",
    "node_intersections.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, *datasets):\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return tuple(d[i] for d in self.datasets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(d) for d in self.datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['base_unnormalized_allcombined', 'Sst-Sstr1', 'Sst-Sstr2', 'Vip-Vipr1', 'Vip-Vipr2'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_loader(datasets, batch_size, shuffle=True, drop_last=True, num_workers=1):\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        ConcatDataset(*[datasets[k][2] for k in datasets.keys()]),\n",
    "        batch_size=batch_size, \n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers)\n",
    "    return {k:i for i,k in enumerate(datasets.keys())}, data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "arm_keys, data_loader = build_data_loader(datasets, batch_size=2000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 5, 20, 41,  ..., 65, 70, 65]), tensor([[20, 41,  0,  0],\n",
      "        [ 5, 41, 87,  0],\n",
      "        [ 5, 20, 87, 65],\n",
      "        ...,\n",
      "        [65, 65, 70, 65],\n",
      "        [65, 65, 65, 70],\n",
      "        [65, 70, 70, 65]])]\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (data1, data2, data3, data4, data4) in enumerate(data_loader):\n",
    "    print(data4)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take care of index in different arms and different number of nodes in different arms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_unnormalized_allcombined': 0,\n",
       " 'Sst-Sstr1': 1,\n",
       " 'Sst-Sstr2': 2,\n",
       " 'Vip-Vipr1': 3,\n",
       " 'Vip-Vipr2': 4}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arm_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9469)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_0 = pd.DataFrame(torch.stack(emb[0]).detach().numpy(), \n",
    "                   index=datasets['base_unnormalized_allcombined'][1].values())\n",
    "\n",
    "v_1 = pd.DataFrame(torch.stack(emb[1]).detach().numpy(), \n",
    "                   index=datasets['Sst-Sstr1'][1].values())\n",
    "\n",
    "v_0.index.name = \"cluster_id\"\n",
    "v_1.index.name = \"cluster_id\"\n",
    "\n",
    "merged = v_1.merge(v_0, on='cluster_id')\n",
    "v_0 = merged[['0_x', '1_x']]\n",
    "v_1 = merged[['0_y', '1_y']]\n",
    "\n",
    "v_0 = torch.tensor(np.array(v_0))\n",
    "v_1 = torch.tensor(np.array(v_1))\n",
    "F.mse_loss(v_0, v_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 base_unnormalized_allcombined 0\n",
      "1 Sst-Sstr1 1\n",
      "2 Vip-Vipr1 2\n"
     ]
    }
   ],
   "source": [
    "loss_joint = 0 \n",
    "\n",
    "base_arm = arm_keys[base_layer_name]\n",
    "for arm, (k, v) in enumerate(arm_keys.items()):\n",
    "    print(arm, k, v)\n",
    "    idx0 = [datasets[base_layer_name][0][i] for i in node_intersections[k]]\n",
    "    idx1 = [datasets[k][0][i] for i in node_intersections[k]]\n",
    "    loss_joint += F.mse_loss(torch.index_select(input=torch.stack(emb[v]), \n",
    "                                                dim=0, \n",
    "                                                index=torch.tensor(idx1), \n",
    "                                                out=None),\n",
    "                             torch.index_select(input=torch.stack(emb[base_arm]), \n",
    "                                                dim=0, \n",
    "                                                index=torch.tensor(idx0), \n",
    "                                                out=None))\n",
    "print(loss_joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_CMCBOW(prediction, target, emb, arm_keys, base_layer_name, node_intersections, n_arm=2):\n",
    "    \n",
    "    base_arm = arm_keys[base_layer_name]\n",
    "    loss_indep = [None] * n_arm\n",
    "    loss_joint = [None] * n_arm\n",
    "    \n",
    "    for arm, (k, v) in enumerate(arm_keys.items()):\n",
    "        \n",
    "        loss_indep[arm] = F.cross_entropy(prediction[arm], target[arm])\n",
    "        \n",
    "        idx0 = [datasets[base_layer_name][0][i] for i in node_intersections[k]]\n",
    "        idx1 = [datasets[k][0][i] for i in node_intersections[k]]\n",
    "        loss_joint[arm] = F.mse_loss(torch.index_select(input=torch.stack(emb[v]), \n",
    "                                                    dim=0, \n",
    "                                                    index=torch.tensor(idx1),\n",
    "                                                    out=None),\n",
    "                                 torch.index_select(input=torch.stack(emb[base_arm]), \n",
    "                                                    dim=0, \n",
    "                                                    index=torch.tensor(idx0), \n",
    "                                                    out=None))\n",
    "    loss = sum(loss_indep) + sum(loss_joint)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2999, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2999, grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "i = 10\n",
    "arm = 2\n",
    "print(F.cross_entropy(predict[arm][[i]], target_data[arm][[i]]))\n",
    "\n",
    "sf = F.softmax(predict[arm][i], dim=0)\n",
    "loss = -1 * torch.log(sf)\n",
    "print(loss[target_data[arm][i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coupled MCBOW_Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CMCBOW_Word2Vec(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size=[93], embedding_size=2, n_arm=1, padding_idx=0):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super(CMCBOW_Word2Vec, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.n_arm = n_arm\n",
    "        \n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(vocab_size[i],\n",
    "                                                      embedding_size, \n",
    "                                                      padding_idx=padding_idx) \n",
    "                                         for i in range(n_arm)])\n",
    "        \n",
    "        self.linear = nn.ModuleList([nn.Linear(embedding_size,\n",
    "                                               vocab_size[i]) \n",
    "                                     for i in range(n_arm)])\n",
    "        \n",
    "        self.batch_norm = nn.ModuleList([nn.BatchNorm1d(num_features=embedding_size,\n",
    "                                                        eps=1e-10, \n",
    "                                                        momentum=0.1, \n",
    "                                                        affine=False) \n",
    "                                         for i in range(n_arm)])\n",
    "                        \n",
    "\n",
    "    def encoder(self, context_words, arm):\n",
    "        h1 = torch.mean(self.embeddings[arm](context_words), dim=1)\n",
    "        node_embeddings = [self.embeddings[arm](torch.tensor(i)) for i \n",
    "                           in range(self.vocab_size[arm])]\n",
    "        return node_embeddings ,h1\n",
    "\n",
    "    def decoder(self, mean_context, arm):\n",
    "        h2 = self.linear[arm](self.batch_norm[arm](mean_context))\n",
    "        return h2\n",
    "\n",
    "    def forward(self, context_words):\n",
    "        emb = [None] * self.n_arm\n",
    "        predictions = [None] * self.n_arm\n",
    "\n",
    "        for arm in range(self.n_arm):\n",
    "            node_embeddings , mean_context = self.encoder(context_words[arm], arm)\n",
    "            emb[arm] = node_embeddings\n",
    "            predictions[arm] = self.decoder(mean_context, arm)\n",
    "            \n",
    "        return emb, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 2\n",
    "learning_rate = 0.001\n",
    "n_epochs = 10\n",
    "n_arm=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time is 86.23\n",
      "epoch: 1/10, loss:26.2459\n",
      "time is 83.36\n",
      "epoch: 2/10, loss:20.0118\n",
      "time is 88.05\n",
      "epoch: 3/10, loss:16.0191\n",
      "time is 86.11\n",
      "epoch: 4/10, loss:13.2241\n",
      "time is 86.24\n",
      "epoch: 5/10, loss:11.2717\n",
      "time is 83.98\n",
      "epoch: 6/10, loss:9.9709\n",
      "time is 83.05\n",
      "epoch: 7/10, loss:9.1404\n",
      "time is 85.70\n",
      "epoch: 8/10, loss:8.6181\n",
      "time is 85.64\n",
      "epoch: 9/10, loss:8.2840\n",
      "time is 92.73\n",
      "epoch: 10/10, loss:8.0616\n"
     ]
    }
   ],
   "source": [
    "model = CMCBOW_Word2Vec(embedding_size=embedding_size, \n",
    "                        vocab_size=[v[3] + 1 for (k, v) in datasets.items()],\n",
    "                        n_arm=n_arm, \n",
    "                        padding_idx=0).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "training_loss = []\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    losses = []\n",
    "    t0 = time.time()\n",
    "    for batch_idx, all_data in enumerate(data_loader):\n",
    "        target_data = [data[0].to(device) for data in all_data]\n",
    "        context_data = [data[1].to(device) for data in all_data]\n",
    "        optimizer.zero_grad()\n",
    "        emb, predict = model(context_data)\n",
    "        loss = loss_CMCBOW(prediction=predict, \n",
    "                           target=target_data, \n",
    "                           arm_keys=arm_keys, \n",
    "                           emb=emb, \n",
    "                           n_arm=n_arm, \n",
    "                           base_layer_name=base_layer_name, \n",
    "                           node_intersections=node_intersections) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    t1 = time.time()\n",
    "    print('time is %.2f' % (t1 - t0))\n",
    "        \n",
    "    training_loss.append(np.mean(losses)) \n",
    "    print(f'epoch: {epoch+1}/{n_epochs}, loss:{np.mean(losses):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([91, 46, 56,  ..., 84, 85, 86]),\n",
       " tensor([30, 30, 30,  ..., 30, 30, 30]),\n",
       " tensor([60, 60, 60,  ..., 60, 60, 60])]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.2212e+00,  1.4648e-01,  4.6296e-01, -8.1406e-01, -7.9877e-01,\n",
       "         7.9398e-02, -1.7338e-02, -5.3285e-02,  4.0035e-01, -4.6990e-01,\n",
       "        -9.0508e-02,  2.4932e-02, -4.7801e-01,  3.3176e-01, -2.6811e-01,\n",
       "         1.6318e-01, -3.5090e-01,  2.2714e-01, -1.0808e-01,  3.8041e-01,\n",
       "        -4.2286e-01,  1.0568e-01,  4.3074e-01, -2.8477e-01, -6.2946e-02,\n",
       "         4.5789e-01,  1.0429e-01,  5.0718e-01,  4.7870e-01, -6.0068e-01,\n",
       "        -3.2283e-01,  6.0317e-02, -7.5750e-02,  3.2986e-01, -8.0745e-01,\n",
       "        -1.2288e-01, -1.6709e-01,  1.2254e-01, -5.9767e-02,  6.6194e-01,\n",
       "        -2.5543e-01, -1.3109e-01, -7.4612e-01, -3.3035e-01, -4.0580e-01,\n",
       "        -6.0866e-02,  4.6160e-01,  1.0293e-01, -9.0680e-02,  3.8162e-01,\n",
       "        -3.0542e-01,  3.0129e-01, -5.2409e-01, -1.0371e+00, -5.6274e-01,\n",
       "        -4.0831e-01, -5.5897e-01,  3.0685e-01,  1.1984e-02, -1.3241e-01,\n",
       "        -1.4466e-01, -4.0001e-01,  2.8036e-01,  6.7843e-01,  6.2587e-01,\n",
       "         1.6307e-01, -5.7426e-01, -4.5189e-01, -3.1672e-02,  5.8077e-02,\n",
       "         5.2959e-01,  1.3562e-01, -5.4268e-01, -1.0024e+00,  2.3856e-01,\n",
       "        -2.1484e-02, -6.6190e-02,  2.2977e-01, -1.5689e-01, -1.3130e-01,\n",
       "         6.6605e-01, -1.7081e-01, -9.2433e-01,  7.3165e-01,  8.0935e-01,\n",
       "        -1.9738e-03,  3.1512e-01, -7.1874e-01, -1.9235e-01, -2.5345e-01,\n",
       "        -1.0101e+00,  4.6454e-01], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading cldf from: //Users/fahimehb/Documents/NPP_GNN_project/dat/cl_df_VISp_annotation.csv\n"
     ]
    }
   ],
   "source": [
    "cldf = utils.read_visp_npp_cldf()\n",
    "vectors = model.embeddings[0].weight.detach().numpy()\n",
    "\n",
    "data = analysis.summarize_walk_embedding_results(gensim_dict={\"model\": vectors},\n",
    "                                                 index=index_2_word.values(),\n",
    "                                                 ndim=2, \n",
    "                                                 cl_df=cldf, \n",
    "                                                 padding_label=\"pad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = utils.get_model_dir(project_name, \n",
    "                                roi, \n",
    "                                N, \n",
    "                                length, \n",
    "                                p, \n",
    "                                q, \n",
    "                                layer_class, \n",
    "                                layer, \n",
    "                                walk_type)\n",
    "\n",
    "model_name = utils.get_model_name(size=embedding_size, \n",
    "                                  iter=n_epochs, \n",
    "                                  window=2, \n",
    "                                  lr=learning_rate, \n",
    "                                  batch_size=batch_size,\n",
    "                                  opt_add=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '//Users/fahimehb/Documents/NPP_GNN_project/models/VISp/single_layer/Directed_Weighted_node2vec/N_1_l_10000_p_1_q_1/Vip-Vipr1/model_size_2_iter_10_window_2_lr_0.001_bs_2000_test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-77ce4d9d94e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py374/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   3226\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3227\u001b[0m         )\n\u001b[0;32m-> 3228\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py374/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             )\n\u001b[1;32m    185\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py374/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0;31m# No explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '//Users/fahimehb/Documents/NPP_GNN_project/models/VISp/single_layer/Directed_Weighted_node2vec/N_1_l_10000_p_1_q_1/Vip-Vipr1/model_size_2_iter_10_window_2_lr_0.001_bs_2000_test.csv'"
     ]
    }
   ],
   "source": [
    "data.to_csv(os.path.join(model_dir, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_size_2_iter_10_window_2_lr_0.001_bs_2000_test.csv'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py374",
   "language": "python",
   "name": "py374"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
